"""
ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå•é¡Œè§£æ±ºã®ãŸã‚ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’é…ä¿¡
"""
import json
import asyncio
from typing import AsyncGenerator, Dict, Any
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from ..services.rag_service import RagService
from ..models.rag_models import RagRequest
from ..core.logging import GameChatLogger
from ..core.performance import bottleneck_detector
import time

router = APIRouter(prefix="/streaming", tags=["streaming"])
logger = GameChatLogger.get_logger(__name__)

class StreamingRagService:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œRAGã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self) -> None:
        self.rag_service = RagService()
    
    async def stream_rag_response(self, rag_req: RagRequest) -> AsyncGenerator[str, None]:
        """
        RAGãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é…ä¿¡
        """
        start_time = time.perf_counter()
        
        try:
            # 1. åˆæœŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆå³åº§ã«è¿”ã™ï¼‰
            yield self._format_chunk({
                "type": "status",
                "message": "æ¤œç´¢ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...",
                "timestamp": time.time()
            })
            
            # 2. æ¤œç´¢å®Ÿè¡Œï¼ˆéåŒæœŸå‡¦ç†ï¼‰
            yield self._format_chunk({
                "type": "status", 
                "message": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ä¸­...",
                "timestamp": time.time()
            })
            
            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®å®Ÿè¡Œ
            search_start = time.perf_counter()
            search_result = await self.rag_service.hybrid_search_service.search(
                rag_req.question, rag_req.top_k or 50
            )
            search_duration = time.perf_counter() - search_start
            
            # æ¤œç´¢çµæœã®é…ä¿¡
            yield self._format_chunk({
                "type": "search_complete",
                "message": f"æ¤œç´¢å®Œäº† ({len(search_result.get('merged_results', []))}ä»¶ã®çµæœ)",
                "search_duration": search_duration,
                "timestamp": time.time()
            })
            
            # 3. LLMå¿œç­”ç”Ÿæˆé–‹å§‹
            yield self._format_chunk({
                "type": "status",
                "message": "AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­...",
                "timestamp": time.time()
            })
            
            # LLMå¿œç­”ã®ç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰
            llm_start = time.perf_counter()
            context_items = search_result.get("merged_results", [])[:10]
            
            async for response_chunk in self._stream_llm_response(
                rag_req.question, context_items
            ):
                yield response_chunk
            
            llm_duration = time.perf_counter() - llm_start
            total_duration = time.perf_counter() - start_time
            
            # 4. æœ€çµ‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹
            yield self._format_chunk({
                "type": "complete",
                "message": "å›ç­”ç”Ÿæˆå®Œäº†",
                "performance": {
                    "total_duration": total_duration,
                    "search_duration": search_duration,
                    "llm_duration": llm_duration
                },
                "timestamp": time.time()
            })
            
            # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º
            bottleneck_detector.check_operation(
                "streaming_rag_total", total_duration,
                {"question_length": len(rag_req.question)}
            )
            
        except Exception as e:
            logger.error(f"Streaming error: {e}")
            yield self._format_chunk({
                "type": "error",
                "message": f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                "timestamp": time.time()
            })
    
    async def _stream_llm_response(self, question: str, context_items: list) -> AsyncGenerator[str, None]:
        """LLMå¿œç­”ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é…ä¿¡"""
        try:
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
            context_text = "\n".join([
                f"ã€{item.title}ã€‘\n{item.text}"
                for item in context_items[:5]  # ä¸Šä½5ä»¶ã®ã¿ä½¿ç”¨
            ])
            
            # OpenAI ChatCompletionã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
            response_stream = self.rag_service.llm_service.stream_response(
                question, context_text
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§é…ä¿¡
            accumulated_response = ""
            async for chunk in response_stream:
                if chunk:
                    accumulated_response += chunk
                    yield self._format_chunk({
                        "type": "llm_chunk",
                        "content": chunk,
                        "accumulated": accumulated_response,
                        "timestamp": time.time()
                    })
                    
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹é…ä¿¡ã®èª¿æ•´ï¼ˆéè² è·é˜²æ­¢ï¼‰
                    await asyncio.sleep(0.01)
            
        except Exception as e:
            logger.error(f"LLM streaming error: {e}")
            yield self._format_chunk({
                "type": "llm_error",
                "message": f"LLMå¿œç­”ã‚¨ãƒ©ãƒ¼: {str(e)}",
                "timestamp": time.time()
            })
    
    def _format_chunk(self, data: Dict[str, Any]) -> str:
        """SSEå½¢å¼ã§ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

streaming_service = StreamingRagService()

@router.post("/rag/query")
async def stream_rag_query(rag_req: RagRequest) -> StreamingResponse:
    """
    ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œRAGã‚¯ã‚¨ãƒªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    Server-Sent Events (SSE) ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é…ä¿¡
    """
    if not rag_req.question:
        raise HTTPException(status_code=400, detail="è³ªå•ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    logger.info(f"ğŸš€ Streaming RAG query: {rag_req.question[:50]}...")
    
    async def event_generator() -> AsyncGenerator[str, None]:
        async for chunk in streaming_service.stream_rag_response(rag_req):
            yield chunk
    
    return StreamingResponse(
        event_generator(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Nginxãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ç„¡åŠ¹åŒ–
        }
    )

@router.get("/health")
async def streaming_health() -> Dict[str, Any]:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "healthy",
        "service": "streaming",
        "timestamp": time.time()
    }
