from typing import Dict, Any, Optional
from ..models.rag_models import RagRequest
from .embedding_service import EmbeddingService
from .vector_service import VectorService
from .llm_service import LLMService
from .hybrid_search_service import HybridSearchService
from ..config.ng_words import NG_WORDS
from ..core.cache import prewarmed_query_cache as query_cache
import logging
import time
import asyncio

logger = logging.getLogger(__name__)

class RagService:
    def __init__(self) -> None:
        self.embedding_service = EmbeddingService()
        self.vector_service = VectorService()
        self.llm_service = LLMService()
        self.hybrid_search_service = HybridSearchService()
    
    async def process_query(self, rag_req: RagRequest) -> Dict[str, Any]:
        """
        RAGã‚¯ã‚¨ãƒªã‚’å‡¦ç†ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ã
        """
        import sys
        start_time = time.perf_counter()
        print(f"[RAG] process_query called: question='{rag_req.question}', top_k={rag_req.top_k}", file=sys.stderr)
        
        try:
            # NGãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            if any(ng_word in rag_req.question for ng_word in NG_WORDS):
                print("[RAG] NGãƒ¯ãƒ¼ãƒ‰æ¤œå‡º: abort", file=sys.stderr)
                return {
                    "answer": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ãã®ã‚ˆã†ãªå†…å®¹ã«ã¯ãŠç­”ãˆã§ãã¾ã›ã‚“ã€‚"
                }

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¿œç­”ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆé«˜é€ŸåŒ–ï¼‰
            cache_check_start = time.perf_counter()
            cached_response = await query_cache.get_cached_response(
                rag_req.question, rag_req.top_k or 50
            )
            cache_check_duration = time.perf_counter() - cache_check_start
            
            if cached_response:
                print(f"[RAG] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {cache_check_duration:.3f}s", file=sys.stderr)
                return cached_response

            # æ¤œç´¢å®Ÿè¡Œ
            search_start = time.perf_counter()
            optimized_top_k = rag_req.top_k or 50
            search_result = await self.hybrid_search_service.search(rag_req.question, optimized_top_k)
            search_duration = time.perf_counter() - search_start
            
            # LLMå‡¦ç†ï¼ˆä»Šå›ã¯ç©ºæ–‡å­—ï¼‰
            llm_start = time.perf_counter()
            llm_duration = time.perf_counter() - llm_start  # 0ç§’
            
            total_duration = time.perf_counter() - start_time
            
            logger.info(f"â±ï¸ RAGå‡¦ç†å®Œäº†: total={total_duration:.3f}s, search={search_duration:.3f}s, llm={llm_duration:.3f}s")
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±å‡ºåŠ›
            classification = search_result.get("classification")
            query_type = getattr(classification, "query_type", None) if classification else None
            db_results = search_result.get("db_results", [])
            
            print(f"[RAG][DEBUG] query_type={query_type}, db_results_count={len(db_results)}", file=sys.stderr)
            logger.info(f"[RAG][DEBUG] query_type={query_type}, db_results_count={len(db_results)}")
            
            # ãƒ‡ãƒãƒƒã‚°: db_resultsã®å†…å®¹ã‚’ãƒ­ã‚°å‡ºåŠ›
            if db_results:
                def get_name_safe(item: Any) -> str:
                    if isinstance(item, dict):
                        return str(item.get('name', str(item)))
                    return str(item)

                db_names = [get_name_safe(item) for item in db_results]
                print(f"[RAG][DEBUG] db_results(full): {db_names}", file=sys.stderr)
                logger.info(f"[RAG][DEBUG] db_results(full): {db_names}")


            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰
            if rag_req.with_context:
                # contextã‚’è©³ç´°JSONã«å¤‰æ›
                if str(query_type) == "QueryType.FILTERABLE" or str(query_type).lower() == "filterable":
                    # FILTERABLEã®å ´åˆï¼šDBæ¤œç´¢çµæœã‚’è©³ç´°JSONã§è¿”ã™
                    db_card_names = [item if isinstance(item, str) else str(item) for item in db_results]
                    card_details = self.hybrid_search_service.database_service.get_card_details_by_titles(db_card_names)
                    
                    response = {
                        "answer": "",
                        "context": card_details,  # ã‚«ãƒ¼ãƒ‰è©³ç´°JSONãƒªã‚¹ãƒˆ
                        "db_results": card_details,  # db_resultsã«ã‚‚è©³ç´°jsonãƒªã‚¹ãƒˆã‚’æ ¼ç´
                        "classification": classification.model_dump() if classification and hasattr(classification, "model_dump") else ({} if classification is None else dict(classification)),
                        "search_info": {
                            "query_type": str(query_type).lower() if query_type else "unknown",
                            "confidence": getattr(classification, "confidence", 0.0) if classification else 0.0,
                            "db_results_count": len(card_details),
                            "vector_results_count": len(search_result.get("vector_results", []))
                        },
                        "performance": {
                            "total_duration": total_duration,
                            "search_duration": search_duration,
                            "llm_duration": llm_duration,
                            "cache_hit": False
                        }
                    }
                    print("[RAG][DEBUG] FILTERABLE: returning card details JSON list", file=sys.stderr)
                    logger.info("[RAG][DEBUG] FILTERABLE: returning card details JSON list")
                else:
                    # FILTERABLEä»¥å¤–ï¼šcontextã‹ã‚‰è©³ç´°JSONå–å¾—
                    context_items = search_result.get("context", [])
                    card_details = []
                    
                    for item in context_items:
                        if isinstance(item, dict):
                            card_details.append(item)
                        elif isinstance(item, str):
                            # ã‚«ãƒ¼ãƒ‰åã®å ´åˆã¯è©³ç´°ã‚’å–å¾—
                            details = self.hybrid_search_service.database_service.get_card_details_by_titles([item])
                            if details:
                                card_details.extend(details)
                            else:
                                # è©³ç´°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ææ¡ˆã¨ã—ã¦è¿½åŠ 
                                card_details.append({
                                    "name": "ã”ææ¡ˆ",
                                    "type": "info", 
                                    "content": item,
                                    "is_suggestion": True
                                })
                    
                    response = {
                        "answer": "",
                        "context": card_details,  # ã‚«ãƒ¼ãƒ‰è©³ç´°JSONãƒªã‚¹ãƒˆ
                        "classification": classification.model_dump() if classification and hasattr(classification, "model_dump") else ({} if classification is None else dict(classification)),
                        "search_info": {
                            "query_type": str(query_type).lower() if query_type else "unknown",
                            "confidence": getattr(classification, "confidence", 0.0) if classification else 0.0,
                            "db_results_count": len(search_result.get("db_results", [])),
                            "vector_results_count": len(search_result.get("vector_results", []))
                        },
                        "performance": {
                            "total_duration": total_duration,
                            "search_duration": search_duration,
                            "llm_duration": llm_duration,
                            "cache_hit": False
                        }
                    }
                    print("[RAG][DEBUG] Not FILTERABLE: returning context card details JSON list", file=sys.stderr)
                    logger.info("[RAG][DEBUG] Not FILTERABLE: returning context card details JSON list")
            else:
                # with_context=Falseã®å ´åˆ
                response = {
                    "answer": "",
                    "performance": {
                        "total_duration": total_duration,
                        "search_duration": search_duration,
                        "llm_duration": llm_duration,
                        "cache_hit": False
                    }
                }
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆéåŒæœŸã§å®Ÿè¡Œã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã«å½±éŸ¿ã—ãªã„ï¼‰
            asyncio.create_task(
                query_cache.cache_response(
                    rag_req.question, 
                    response, 
                    optimized_top_k,
                    ttl=1200 if total_duration < 3.0 else 600  # é«˜é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯é•·æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥
                )
            )
            print("[RAG] process_query done", file=sys.stderr)
            return response
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’å‡ºåŠ›ï¼ˆraiseã—ãªã„ã®ã§logger.errorã®ã¿ã§OKï¼‰
            print(f"[RAG] ERROR: {str(e)}", file=sys.stderr)
            logger.error(f"RAGã‚¯ã‚¨ãƒªå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}", exc_info=True)
            return {
                "answer": f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã€Œ{rag_req.question}ã€ã«é–¢ã™ã‚‹å›ç­”ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
            }
    
    async def process_query_optimized(self, rag_req: RagRequest) -> Dict[str, Any]:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸRAGã‚¯ã‚¨ãƒªå‡¦ç†
        - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç©æ¥µæ´»ç”¨
        - ä¸¦åˆ—å‡¦ç†ã®æœ€é©åŒ–
        - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–
        """
        start_time = time.perf_counter()
        try:
            # NGãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            if any(ng_word in rag_req.question for ng_word in NG_WORDS):
                return {
                    "answer": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ãã®ã‚ˆã†ãªå†…å®¹ã«ã¯ãŠç­”ãˆã§ãã¾ã›ã‚“ã€‚"
                }

            # 1. ãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cached_response = await self._check_multilevel_cache(rag_req)
            if cached_response:
                cache_duration = time.perf_counter() - start_time
                logger.info(f"ğŸš€ Multi-level cache hit: {rag_req.question[:50]}... ({cache_duration:.3f}s)")
                cached_response["performance"]["cache_hit"] = True
                return cached_response

            # 2. ä¸¦åˆ—æ¤œç´¢å®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            search_result = await self._execute_parallel_search(rag_req)
            search_duration = search_result.get("_search_duration", 0)

            # 3. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œLLMå¿œç­”ç”Ÿæˆ
            llm_start = time.perf_counter()
            answer = await asyncio.wait_for(
                self._generate_answer_with_timeout(rag_req, search_result),
                timeout=10.0  # 10ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            )
            llm_duration = time.perf_counter() - llm_start

            # 4. ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            response = await self._build_and_cache_response(
                rag_req, answer, search_result, start_time, search_duration, llm_duration
            )
            return response

        except asyncio.TimeoutError:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            logger.warning(f"â° Query timeout: {rag_req.question[:50]}...")
            return {
                "answer": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆã«æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã¦ã„ã¾ã™ã€‚ã‚‚ã†å°‘ã—å…·ä½“çš„ãªè³ªå•ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚",
                "performance": {
                    "total_duration": time.perf_counter() - start_time,
                    "timeout": True
                }
            }
        except Exception as e:
            logger.error(f"RAGå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "answer": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾Œã«å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚",
                "error": str(e),
                "performance": {
                    "total_duration": time.perf_counter() - start_time,
                    "error": True
                }
            }
    
    async def _check_multilevel_cache(self, rag_req: RagRequest) -> Optional[Dict[str, Any]]:
        """ãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯"""
        # ãƒ¬ãƒ™ãƒ«1: å®Œå…¨ä¸€è‡´ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        exact_cache = await query_cache.get_cached_response(
            rag_req.question, rag_req.top_k or 50
        )
        if exact_cache:
            return exact_cache
        
        # ãƒ¬ãƒ™ãƒ«2: é¡ä¼¼ã‚¯ã‚¨ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
        similar_cache = await self._find_similar_cached_response(rag_req.question)
        if similar_cache:
            return similar_cache
        
        return None
    
    async def _find_similar_cached_response(self, question: str) -> Optional[Dict[str, Any]]:
        """é¡ä¼¼ã‚¯ã‚¨ãƒªã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œç´¢"""
        # ç°¡æ˜“å®Ÿè£…ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼æ€§ãƒã‚§ãƒƒã‚¯
        # question_words = set(question.lower().split())
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‹ã‚‰éå»ã®ã‚¯ã‚¨ãƒªã‚’å–å¾—ï¼ˆå®Ÿè£…ç°¡ç•¥åŒ–ï¼‰
        # å®Ÿéš›ã®ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã‚ˆã‚Šé«˜åº¦ãªé¡ä¼¼åº¦è¨ˆç®—ãŒå¿…è¦
        return None
    
    async def _execute_parallel_search(self, rag_req: RagRequest) -> Dict[str, Any]:
        """ä¸¦åˆ—æ¤œç´¢å®Ÿè¡Œ"""
        search_start = time.perf_counter()
        
        try:
            # è»½é‡ãªã‚¯ã‚¨ãƒªåˆ†é¡ã‚’ä¸¦åˆ—å®Ÿè¡Œï¼ˆå°†æ¥ä½¿ç”¨ï¼‰
            # classification_task = asyncio.create_task(
            #     self.hybrid_search_service.classification_service.classify_query_lightweight(
            #         rag_req.question
            #     )
            # )
            
            # æ¤œç´¢å‡¦ç†ã‚’ä¸¦åˆ—å®Ÿè¡Œ
            search_task = asyncio.create_task(
                asyncio.wait_for(
                    self.hybrid_search_service.search(rag_req.question, rag_req.top_k or 50),
                    timeout=15.0  # æ¤œç´¢ã¯15ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                )
            )
            
            # ä¸¡æ–¹ã®å®Œäº†ã‚’å¾…æ©Ÿ
            search_result = await search_task
            search_result["_search_duration"] = time.perf_counter() - search_start
            
            return search_result
            
        except asyncio.TimeoutError:
            logger.warning(f"â° Search timeout: {rag_req.question[:50]}...")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è»½é‡ãªæ¤œç´¢çµæœã‚’è¿”ã™
            return {
                "merged_results": [],
                "classification": None,
                "_search_duration": time.perf_counter() - search_start,
                "_timeout": True
            }
    
    async def _generate_answer_with_timeout(self, rag_req: RagRequest, search_result: Dict[str, Any]) -> str:
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾å¿œLLMå¿œç­”ç”Ÿæˆ"""
        context_items = search_result.get("context", [])  # è©³ç´°jsonãƒªã‚¹ãƒˆ
        
        if not context_items or search_result.get("_timeout"):
            # æ¤œç´¢çµæœãŒç„¡ã„ã€ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return f"ã€Œ{rag_req.question}ã€ã«ã¤ã„ã¦ã€ç¾åœ¨æ¤œç´¢çµæœã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†å°‘ã—å…·ä½“çš„ãªè³ªå•ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚"
        
        # LLMå¿œç­”ç”Ÿæˆï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã¨ã—ã¦ã‚‚å®Ÿè¡Œå¯èƒ½ï¼‰
        answer = await self.llm_service.generate_answer(
            query=rag_req.question,
            context_items=context_items[:5],  # ä¸Šä½5ä»¶ã®ã¿ä½¿ç”¨
            classification=search_result.get("classification")
        )
        
        return answer
    
    async def _build_and_cache_response(
        self, 
        rag_req: RagRequest, 
        answer: str, 
        search_result: Dict[str, Any],
        start_time: float,
        search_duration: float,
        llm_duration: float
    ) -> Dict[str, Any]:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        total_duration = time.perf_counter() - start_time
        
        logger.info(
            f"â±ï¸ RAGå‡¦ç†å®Œäº†: total={total_duration:.3f}s, "
            f"search={search_duration:.3f}s, llm={llm_duration:.3f}s"
        )
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰
        if rag_req.with_context:
            classification = search_result.get("classification")
            query_type = getattr(classification, "query_type", None) if classification else None
            
            # contextã‚’è©³ç´°JSONã«å¤‰æ›
            if str(query_type) == "QueryType.FILTERABLE" or str(query_type).lower() == "filterable":
                # FILTERABLEã®å ´åˆï¼šDBæ¤œç´¢çµæœã‚’è©³ç´°JSONã§è¿”ã™
                db_results = search_result.get("db_results", [])
                db_card_names = [item if isinstance(item, str) else str(item) for item in db_results]
                card_details = self.hybrid_search_service.database_service.get_card_details_by_titles(db_card_names)
                
                response = {
                    "answer": answer,
                    "context": card_details,  # ã‚«ãƒ¼ãƒ‰è©³ç´°JSONãƒªã‚¹ãƒˆ
                    "db_results": card_details,  # ã‚«ãƒ¼ãƒ‰è©³ç´°JSONãƒªã‚¹ãƒˆ
                    "classification": classification.model_dump() if classification and hasattr(classification, "model_dump") else ({} if classification is None else dict(classification)),
                    "search_info": {
                        "query_type": str(query_type).lower() if query_type else "unknown",
                        "confidence": getattr(classification, "confidence", 0.0) if classification else 0.0,
                        "db_results_count": len(card_details),
                        "vector_results_count": len(search_result.get("vector_results", []))
                    },
                    "performance": {
                        "total_duration": total_duration,
                        "search_duration": search_duration,
                        "llm_duration": llm_duration,
                        "cache_hit": False
                    }
                }
            else:
                # FILTERABLEä»¥å¤–ï¼šcontextã‹ã‚‰è©³ç´°JSONå–å¾—
                context_items = search_result.get("context", [])
                card_details = []
                
                for item in context_items:
                    if isinstance(item, dict):
                        card_details.append(item)
                    elif isinstance(item, str):
                        # ã‚«ãƒ¼ãƒ‰åã®å ´åˆã¯è©³ç´°ã‚’å–å¾—
                        details = self.hybrid_search_service.database_service.get_card_details_by_titles([item])
                        if details:
                            card_details.extend(details)
                        else:
                            # è©³ç´°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ææ¡ˆã¨ã—ã¦è¿½åŠ 
                            card_details.append({
                                "name": "ã”ææ¡ˆ",
                                "type": "info", 
                                "content": item,
                                "is_suggestion": True
                            })
                
                response = {
                    "answer": answer,
                    "context": card_details,  # ã‚«ãƒ¼ãƒ‰è©³ç´°JSONãƒªã‚¹ãƒˆ
                    "classification": classification.model_dump() if classification and hasattr(classification, "model_dump") else ({} if classification is None else dict(classification)),
                    "search_info": {
                        "query_type": str(query_type).lower() if query_type else "unknown",
                        "confidence": getattr(classification, "confidence", 0.0) if classification else 0.0,
                        "db_results_count": len(search_result.get("db_results", [])),
                        "vector_results_count": len(search_result.get("vector_results", []))
                    },
                    "performance": {
                        "total_duration": total_duration,
                        "search_duration": search_duration,
                        "llm_duration": llm_duration,
                        "cache_hit": False
                    }
                }
        else:
            response = {
                "answer": answer,
                "performance": {
                    "total_duration": total_duration,
                    "search_duration": search_duration,
                    "llm_duration": llm_duration,
                    "cache_hit": False
                }
            }
        
        # é«˜é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯é•·æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€é…ã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯çŸ­æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥
        cache_ttl = 1200 if total_duration < 3.0 else 300  # 20åˆ† or 5åˆ†
        
        # éåŒæœŸã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã«å½±éŸ¿ã—ãªã„ï¼‰
        asyncio.create_task(
            query_cache.cache_response(
                rag_req.question, 
                response, 
                rag_req.top_k or 50,
                ttl=cache_ttl
            )
        )
        
        return response