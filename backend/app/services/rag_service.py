from typing import Dict, Any, Optional
from ..models.rag_models import RagRequest
from .embedding_service import EmbeddingService
from .vector_service import VectorService
from .llm_service import LLMService
from .hybrid_search_service import HybridSearchService
from ..config.ng_words import NG_WORDS
from ..core.performance import bottleneck_detector
from ..core.cache import prewarmed_query_cache as query_cache
import logging
import time
import asyncio

logger = logging.getLogger(__name__)

class RagService:
    def __init__(self) -> None:
        self.embedding_service = EmbeddingService()
        self.vector_service = VectorService()
        self.llm_service = LLMService()
        self.hybrid_search_service = HybridSearchService()
    
    async def process_query(self, rag_req: RagRequest) -> Dict[str, Any]:
        """
        RAGã‚¯ã‚¨ãƒªã‚’å‡¦ç†ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ã
        """
        start_time = time.perf_counter()
        
        try:
            # NGãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            if any(ng_word in rag_req.question for ng_word in NG_WORDS):
                return {
                    "answer": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ãã®ã‚ˆã†ãªå†…å®¹ã«ã¯ãŠç­”ãˆã§ãã¾ã›ã‚“ã€‚"
                }

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¿œç­”ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆé«˜é€ŸåŒ–ï¼‰
            cache_check_start = time.perf_counter()
            cached_response = await query_cache.get_cached_response(
                rag_req.question, rag_req.top_k or 50
            )
            cache_check_duration = time.perf_counter() - cache_check_start
            
            if cached_response:
                total_duration = time.perf_counter() - start_time
                logger.info(f"ğŸš€ Cache hit: {rag_req.question[:50]}... ({total_duration:.3f}s)")
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚ã®æœ€å°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
                cached_response["performance"]["total_duration"] = total_duration
                cached_response["performance"]["cache_check_duration"] = cache_check_duration
                return cached_response

            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®å®Ÿè¡Œï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
            search_start = time.perf_counter()
            
            # å‹•çš„ãªtop_kèª¿æ•´ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰
            optimized_top_k = rag_req.top_k or 50
            if optimized_top_k > 30:
                optimized_top_k = min(30, optimized_top_k)  # æœ€å¤§30ã«åˆ¶é™
            
            search_result = await self.hybrid_search_service.search(
                rag_req.question, optimized_top_k
            )
            search_duration = time.perf_counter() - search_start
            
            # 5ç§’ä»¥ä¸Šã®å ´åˆã¯è­¦å‘Šï¼ˆVectoræ¤œç´¢æœ€é©åŒ–ã®ãŸã‚ï¼‰
            if search_duration > 4.0:
                logger.warning(f"âš ï¸ Slow search detected: {search_duration:.3f}s for '{rag_req.question[:50]}...'")
                bottleneck_detector.check_operation(
                    "hybrid_search",
                    search_duration,
                    {"question": rag_req.question[:100], "top_k": optimized_top_k}
                )
            
            context_items = search_result["merged_results"]  # ã“ã“ã¯è©³ç´°json(dict)ãƒªã‚¹ãƒˆ
            
            # LLMå¿œç­”ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€answerã¯ç©ºæ–‡å­—ã§è¿”ã™
            llm_duration = 0.0
            total_duration = time.perf_counter() - start_time
            logger.info(
                f"â±ï¸ RAGå‡¦ç†å®Œäº†: total={total_duration:.3f}s, "
                f"search={search_duration:.3f}s, llm={llm_duration:.3f}s"
            )

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰
            if rag_req.with_context:
                response = {
                    "answer": "",
                    "context": context_items,  # ãã®ã¾ã¾è¿”ã™
                    "classification": search_result["classification"].model_dump(),
                    "search_info": {
                        "query_type": search_result["classification"].query_type,
                        "confidence": search_result["classification"].confidence,
                        "db_results_count": len(search_result["db_results"]),
                        "vector_results_count": len(search_result["vector_results"])
                    },
                    "performance": {
                        "total_duration": total_duration,
                        "search_duration": search_duration,
                        "llm_duration": llm_duration,
                        "cache_hit": False
                    }
                }
            else:
                response = {
                    "answer": "",
                    "performance": {
                        "total_duration": total_duration,
                        "search_duration": search_duration,
                        "llm_duration": llm_duration,
                        "cache_hit": False
                    }
                }
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆéåŒæœŸã§å®Ÿè¡Œã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã«å½±éŸ¿ã—ãªã„ï¼‰
            asyncio.create_task(
                query_cache.cache_response(
                    rag_req.question, 
                    response, 
                    optimized_top_k,
                    ttl=1200 if total_duration < 3.0 else 600  # é«˜é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯é•·æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥
                )
            )
            
            return response
                
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’å‡ºåŠ›ï¼ˆraiseã—ãªã„ã®ã§logger.errorã®ã¿ã§OKï¼‰
            logger.error(f"RAGã‚¯ã‚¨ãƒªå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}", exc_info=True)
            return {
                "answer": f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã€Œ{rag_req.question}ã€ã«é–¢ã™ã‚‹å›ç­”ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
            }
    
    async def process_query_optimized(self, rag_req: RagRequest) -> Dict[str, Any]:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸRAGã‚¯ã‚¨ãƒªå‡¦ç†
        - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç©æ¥µæ´»ç”¨
        - ä¸¦åˆ—å‡¦ç†ã®æœ€é©åŒ–
        - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–
        """
        start_time = time.perf_counter()
        
        try:
            # NGãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            if any(ng_word in rag_req.question for ng_word in NG_WORDS):
                return {
                    "answer": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ãã®ã‚ˆã†ãªå†…å®¹ã«ã¯ãŠç­”ãˆã§ãã¾ã›ã‚“ã€‚"
                }

            # 1. ãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cached_response = await self._check_multilevel_cache(rag_req)
            if cached_response:
                cache_duration = time.perf_counter() - start_time
                logger.info(f"ğŸš€ Multi-level cache hit: {rag_req.question[:50]}... ({cache_duration:.3f}s)")
                cached_response["performance"]["cache_hit"] = True
                return cached_response

            # 2. ä¸¦åˆ—æ¤œç´¢å®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            search_result = await self._execute_parallel_search(rag_req)
            search_duration = search_result.get("_search_duration", 0)
            
            # 3. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œLLMå¿œç­”ç”Ÿæˆ
            llm_start = time.perf_counter()
            answer = await asyncio.wait_for(
                self._generate_answer_with_timeout(rag_req, search_result),
                timeout=10.0  # 10ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            )
            llm_duration = time.perf_counter() - llm_start

            # 4. ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            response = await self._build_and_cache_response(
                rag_req, answer, search_result, start_time, search_duration, llm_duration
            )
            
            return response
                
        except asyncio.TimeoutError:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            logger.warning(f"â° Query timeout: {rag_req.question[:50]}...")
            return {
                "answer": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å›ç­”ã®ç”Ÿæˆã«æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã¦ã„ã¾ã™ã€‚ã‚‚ã†å°‘ã—å…·ä½“çš„ãªè³ªå•ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚",
                "performance": {
                    "total_duration": time.perf_counter() - start_time,
                    "timeout": True
                }
            }
        except Exception as e:
            logger.error(f"RAGå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "answer": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾Œã«å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚",
                "error": str(e),
                "performance": {
                    "total_duration": time.perf_counter() - start_time,
                    "error": True
                }
            }
    
    async def _check_multilevel_cache(self, rag_req: RagRequest) -> Optional[Dict[str, Any]]:
        """ãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯"""
        # ãƒ¬ãƒ™ãƒ«1: å®Œå…¨ä¸€è‡´ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        exact_cache = await query_cache.get_cached_response(
            rag_req.question, rag_req.top_k or 50
        )
        if exact_cache:
            return exact_cache
        
        # ãƒ¬ãƒ™ãƒ«2: é¡ä¼¼ã‚¯ã‚¨ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
        similar_cache = await self._find_similar_cached_response(rag_req.question)
        if similar_cache:
            return similar_cache
        
        return None
    
    async def _find_similar_cached_response(self, question: str) -> Optional[Dict[str, Any]]:
        """é¡ä¼¼ã‚¯ã‚¨ãƒªã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œç´¢"""
        # ç°¡æ˜“å®Ÿè£…ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼æ€§ãƒã‚§ãƒƒã‚¯
        # question_words = set(question.lower().split())
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‹ã‚‰éå»ã®ã‚¯ã‚¨ãƒªã‚’å–å¾—ï¼ˆå®Ÿè£…ç°¡ç•¥åŒ–ï¼‰
        # å®Ÿéš›ã®ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã‚ˆã‚Šé«˜åº¦ãªé¡ä¼¼åº¦è¨ˆç®—ãŒå¿…è¦
        return None
    
    async def _execute_parallel_search(self, rag_req: RagRequest) -> Dict[str, Any]:
        """ä¸¦åˆ—æ¤œç´¢å®Ÿè¡Œ"""
        search_start = time.perf_counter()
        
        try:
            # è»½é‡ãªã‚¯ã‚¨ãƒªåˆ†é¡ã‚’ä¸¦åˆ—å®Ÿè¡Œï¼ˆå°†æ¥ä½¿ç”¨ï¼‰
            # classification_task = asyncio.create_task(
            #     self.hybrid_search_service.classification_service.classify_query_lightweight(
            #         rag_req.question
            #     )
            # )
            
            # æ¤œç´¢å‡¦ç†ã‚’ä¸¦åˆ—å®Ÿè¡Œ
            search_task = asyncio.create_task(
                asyncio.wait_for(
                    self.hybrid_search_service.search(rag_req.question, rag_req.top_k or 50),
                    timeout=15.0  # æ¤œç´¢ã¯15ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                )
            )
            
            # ä¸¡æ–¹ã®å®Œäº†ã‚’å¾…æ©Ÿ
            search_result = await search_task
            search_result["_search_duration"] = time.perf_counter() - search_start
            
            return search_result
            
        except asyncio.TimeoutError:
            logger.warning(f"â° Search timeout: {rag_req.question[:50]}...")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è»½é‡ãªæ¤œç´¢çµæœã‚’è¿”ã™
            return {
                "merged_results": [],
                "classification": None,
                "_search_duration": time.perf_counter() - search_start,
                "_timeout": True
            }
    
    async def _generate_answer_with_timeout(self, rag_req: RagRequest, search_result: Dict[str, Any]) -> str:
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾å¿œLLMå¿œç­”ç”Ÿæˆ"""
        context_items = search_result.get("merged_results", [])  # è©³ç´°jsonãƒªã‚¹ãƒˆ
        
        if not context_items or search_result.get("_timeout"):
            # æ¤œç´¢çµæœãŒç„¡ã„ã€ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return f"ã€Œ{rag_req.question}ã€ã«ã¤ã„ã¦ã€ç¾åœ¨æ¤œç´¢çµæœã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†å°‘ã—å…·ä½“çš„ãªè³ªå•ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚"
        
        # LLMå¿œç­”ç”Ÿæˆï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã¨ã—ã¦ã‚‚å®Ÿè¡Œå¯èƒ½ï¼‰
        answer = await self.llm_service.generate_answer(
            query=rag_req.question,
            context_items=context_items[:5],  # ä¸Šä½5ä»¶ã®ã¿ä½¿ç”¨
            classification=search_result.get("classification")
        )
        
        return answer
    
    async def _build_and_cache_response(
        self, 
        rag_req: RagRequest, 
        answer: str, 
        search_result: Dict[str, Any],
        start_time: float,
        search_duration: float,
        llm_duration: float
    ) -> Dict[str, Any]:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        total_duration = time.perf_counter() - start_time
        
        logger.info(
            f"â±ï¸ RAGå‡¦ç†å®Œäº†: total={total_duration:.3f}s, "
            f"search={search_duration:.3f}s, llm={llm_duration:.3f}s"
        )
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰
        context_items = search_result.get("merged_results", [])  # è©³ç´°jsonãƒªã‚¹ãƒˆ
        
        if rag_req.with_context:
            response = {
                "answer": "",
                "context": context_items[:10],  # ä¸Šä½10ä»¶
                "classification": search_result.get("classification", {}).model_dump() if search_result.get("classification") else {},
                "search_info": {
                    "query_type": search_result.get("classification", {}).query_type if search_result.get("classification") else "unknown",
                    "confidence": search_result.get("classification", {}).confidence if search_result.get("classification") else 0.0,
                    "db_results_count": len(search_result.get("db_results", [])),
                    "vector_results_count": len(search_result.get("vector_results", []))
                },
                "performance": {
                    "total_duration": total_duration,
                    "search_duration": search_duration,
                    "llm_duration": llm_duration,
                    "cache_hit": False
                }
            }
        else:
            response = {
                "answer": "",
                "performance": {
                    "total_duration": total_duration,
                    "search_duration": search_duration,
                    "llm_duration": llm_duration,
                    "cache_hit": False
                }
            }
        
        # é«˜é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯é•·æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€é…ã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯çŸ­æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥
        cache_ttl = 1200 if total_duration < 3.0 else 300  # 20åˆ† or 5åˆ†
        
        # éåŒæœŸã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã«å½±éŸ¿ã—ãªã„ï¼‰
        asyncio.create_task(
            query_cache.cache_response(
                rag_req.question, 
                response, 
                rag_req.top_k or 50,
                ttl=cache_ttl
            )
        )
        
        return response